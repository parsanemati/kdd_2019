{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# original lang\n",
    "# http://scikit-learn.org/stable/modules/multiclass.html\n",
    "# Support multilabel:\n",
    "# http://scikit-learn.org/stable/supervised_learning.html\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "start = time.time()\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import *\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "import datetime\n",
    "#whole clf\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# clf3 = LogisticRegression()\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf4 40 = LogisticRegressionCV(random_state=0)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "# end = time.time()\n",
    "# hours, rem = divmod(end-start, 3600)\n",
    "# minutes, seconds = divmod(rem, 60)\n",
    "# print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "# today = datetime.date.today() # now time\n",
    "# print(today) # today date\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/alinemati/Yandex.Disk/Compitition/2019/KDD2019/Data/Result/new_round_data/\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filename = path+'merged_all_ful_for_traing_rounded_with_no_pid.h5'\n",
    "\n",
    "data_YouTube=pd.read_hdf(filename, 'data_YouTube') # load from H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_YouTube.loc[:, ['date', 'time', 'o_Long', 'o_Lat', 'd_long', 'd_lat', 'distAnce', 'distance_avg', 'distance_max', 'distance_min', 'distance_std', 'price_avg', 'price_max', 'price_std', 'eta_avg', 'eta_max', 'eta_min', 'eta_std', 'mode1', 'mode2', 'mode3', 'mode4', 'mode5', 'mode6', 'mode7']]\n",
    "\n",
    "Y_train = data_YouTube['click']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (400000, 25)\n",
      "x_test: (100000, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Splitting the data into 300 training instances and 104 test instances\n",
    "n = 100000\n",
    "all_Ids = np.arange(len(data_YouTube))\n",
    "# print(all_Ids.shape)\n",
    "# random.shuffle(all_Ids)\n",
    "test_Ids = all_Ids[0:n]\n",
    "train_Ids = all_Ids[n:]\n",
    "data_test = data_YouTube.loc[test_Ids, :]\n",
    "data_train = data_YouTube.loc[train_Ids, :]\n",
    "# print(data_train.head()) , print(data_test.head())\n",
    "\n",
    "# print('done')\n",
    "X_train = data_train.loc[:, ['date', 'time', 'o_Long', 'o_Lat', 'd_long', 'd_lat', 'distAnce', 'distance_avg', 'distance_max', 'distance_min', 'distance_std', 'price_avg', 'price_max', 'price_std', 'eta_avg', 'eta_max', 'eta_min', 'eta_std', 'mode1', 'mode2', 'mode3', 'mode4', 'mode5', 'mode6', 'mode7']]\n",
    "\n",
    "Y_train = data_train['click']\n",
    "\n",
    "\n",
    "x_test = data_test.loc[:, [ 'date', 'time', 'o_Long', 'o_Lat', 'd_long', 'd_lat', 'distAnce', 'distance_avg', 'distance_max', 'distance_min', 'distance_std', 'price_avg', 'price_max', 'price_std', 'eta_avg', 'eta_max', 'eta_min', 'eta_std', 'mode1', 'mode2', 'mode3', 'mode4', 'mode5', 'mode6', 'mode7']]\n",
    "\n",
    "y_test = data_test['click']\n",
    "# print('done')\n",
    "\n",
    "\n",
    "\n",
    "print(\"X_train:\",X_train.shape) , print(\"x_test:\",x_test.shape)\n",
    "\n",
    "\n",
    "# ensemble = MLPClassifier(hidden_layer_sizes=(50), activation='relu'\n",
    "#                           ,max_iter = 1500 , learning_rate=\"constant\", \n",
    "#                      shuffle=False, solver='adam' , random_state=0 , warm_start=True, tol=0.0001)\n",
    "\n",
    "\n",
    "# ensemble=BaggingClassifier()\n",
    "# clf2=XGBClassifier()\n",
    "\n",
    "\n",
    "# ensemble = VotingClassifier(estimators=[\n",
    "#                                     ('clf0', clf0) , \n",
    "#                                     ('clf1', clf1) , \n",
    "#                                     ('clf2', clf2), \n",
    "# #                                     ('clf3', clf3), \n",
    "# #                                     ('clf4', clf4), \n",
    "# #                                     (\"clf5\" , clf5), \n",
    "# #                                     (\"clf6\" , clf6), \n",
    "#                                    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_with_mode = data_test.loc[:, ['click' , 'mode1', 'mode2', 'mode3', 'mode4', 'mode5', 'mode6', 'mode7']]\n",
    "y_test_with_mode.to_csv(\"y_test100000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_weighted_cross_validate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn.metrics import *\n",
    "import datetime, time\n",
    "start = time.time()\n",
    "\n",
    "def mse(X, Y):\n",
    "    MSE=mean_squared_error(X, Y)\n",
    "    return print(\"MSE:\" , MSE)\n",
    "\n",
    "def R2(X, Y):\n",
    "    r_2=r2_score(X, Y)\n",
    "    return print(\"r_2:\" , r_2)\n",
    "\n",
    "def f1_weighted_cross_validate_report(model ,X, Y , cv):\n",
    "    f1=cross_val_score(model ,X, Y, cv=cv ,scoring='f1_weighted' ).mean()\n",
    "    return  print(\"f1_weighted_cross_validate_report:\" , f1 , \" with cv \" , cv)\n",
    "\n",
    "\n",
    "def accuracy_cross_validate_report(model ,X, Y, cv):\n",
    "    acc=cross_val_score(model ,X, Y, cv=cv ,scoring='accuracy' ).mean()\n",
    "    return  print(\"accuracy_cross_validate_report:\", acc , \" with cv \" , cv)\n",
    "\n",
    "\n",
    "def accuracy_real(X,Y):\n",
    "    accuracy_real = accuracy_score(X, Y)*100\n",
    "    return accuracy_real\n",
    "    \n",
    "def  f1score(X , Y): #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "     f1score= f1_score(X , Y, average='weighted')  \n",
    "     return print(\"f1score: \" , f1score)\n",
    "\n",
    "\n",
    "def  recallscore(X,Y ): #tp / (tp + fn)\n",
    "     recall__score= recall__score(X,Y, average='weighted') \n",
    "     return print(\"recall_score: \" , recall_score)\n",
    "\n",
    "                                   \n",
    "def  precisionscore(X , Y ): #tp / (tp + fp)\n",
    "     precision__score= precision_score(X,Y, average='weighted') \n",
    "     return print(\"precision__score: \" , precision__score )\n",
    "\n",
    "def LR_Traning(model, X, Y):\n",
    "    LR_Traning=  model.score(X, Y)\n",
    "    return print(\"LR_Traning:\" , LR_Traning)\n",
    "\n",
    "\n",
    "def prediction(model, X):\n",
    "    predict = model.predict(X)\n",
    "    return print(\"predict\" , predict)\n",
    "\n",
    "\n",
    "from pygame import mixer # Load the required library\n",
    "from threading import Timer\n",
    "\n",
    "def Stop():\n",
    "    mixer.music.stop()\n",
    "\n",
    "def play():\n",
    "    mixer.init()\n",
    "    mixer.music.load('/home/alinemati/Music/Daya - Insomnia.mp3')\n",
    "    mixer.music.play()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def time_spend():\n",
    "    import datetime, time\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    today = datetime.date.today() # now time\n",
    "    print(today) # today date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-18 19:05:04.010995\n",
      "00:06:39.81\n",
      "2019-05-18\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "clf0= XGBClassifier(random_state=0 , shuffle=False  ).fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "time_spend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba=clf0.predict_proba(x_test)\n",
    "predi=clf0.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=pd.DataFrame({'recommend_mode':predi.tolist() })\n",
    "predict.to_csv(\"y_test100000_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 2, 7])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_real(predi ,y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9351,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    0, 10534,   464,  1543,  1070,   465,   813,   528,   117,\n",
       "          272,   102,    96],\n",
       "       [    0,   656, 24006,   653,   377,   141,   226,   271,     4,\n",
       "          309,   276,    16],\n",
       "       [    0,    77,     2,   196,    28,     4,    10,    56,    14,\n",
       "            3,     3,    11],\n",
       "       [    0,     1,     0,     0,    13,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    0,   325,    62,   360,    76,  8623,   686,    18,     0,\n",
       "            0,     0,     0],\n",
       "       [    0,   255,    53,   127,    70,   342,   561,    12,     0,\n",
       "            0,     0,     0],\n",
       "       [    0,   889,   104,  1037,   444,    16,    74, 12121,    69,\n",
       "          490,   484,   102],\n",
       "       [    0,    36,     5,    18,     4,     0,     2,    22,    56,\n",
       "            6,     0,    16],\n",
       "       [    0,   726,  1230,   620,   292,    10,    96,   774,    32,\n",
       "        10280,   190,     0],\n",
       "       [    0,   151,   246,   229,   104,     0,     0,   562,     0,\n",
       "          424,  1840,    19],\n",
       "       [    0,   203,    11,   192,    66,     2,    13,   286,    14,\n",
       "            0,    15,  1100]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(predi ,y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted_cross_validate_report: 0.6749811578620403  with cv  5\n",
      "CR: entropy ,MAXdepth: None ,MAXfeatures: None ,nestimators: 10\n",
      "##############################################\n",
      "f1_weighted_cross_validate_report: 0.6754371291846193  with cv  5\n",
      "CR: entropy ,MAXdepth: None ,MAXfeatures: None ,nestimators: 20\n",
      "##############################################\n",
      "f1_weighted_cross_validate_report: 0.675870969592675  with cv  5\n",
      "CR: entropy ,MAXdepth: None ,MAXfeatures: None ,nestimators: 50\n",
      "##############################################\n",
      "f1_weighted_cross_validate_report: 0.6758624939326816  with cv  5\n",
      "CR: entropy ,MAXdepth: None ,MAXfeatures: None ,nestimators: 100\n",
      "##############################################\n"
     ]
    }
   ],
   "source": [
    "Criterion = [\"entropy\" , \"gini\"]\n",
    "maxfeatures = [None ,\"auto\", \"sqrt\", \"log2\"  ]\n",
    "maxdepth = [None,10 , 20 , 30]\n",
    "nestimators=[100 , 500 ,1000 , 1500 ]\n",
    "\n",
    "# 0.74\n",
    "\n",
    "\n",
    "for CR in Criterion:\n",
    "    for MAXdepth in maxdepth:\n",
    "        for MAXfeatures in maxfeatures:\n",
    "                for nestimators in nestimators:\n",
    "                        clf0=RandomForestClassifier( criterion=CR , max_depth=MAXdepth,\n",
    "                                                  max_features=MAXfeatures,random_state=0,warm_start=True,\n",
    "                                                  n_estimators=nestimators, \n",
    "                                               bootstrap=False)\n",
    " \n",
    "                        eclf1 = clf0.fit(X_train, Y_train)\n",
    "                        f1_weighted_cross_validate_report(clf0, X_train, Y_train , 5)\n",
    "                        print(\"CR:\", CR,\",MAXdepth:\",MAXdepth,\",MAXfeatures:\", MAXfeatures,\",nestimators:\",nestimators )\n",
    "                        print(\"##############################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 17:46:36.518894\n",
      "f1_weighted_cross_validate_report: 0.7230806482358538  with cv  5\n",
      "00:49:50.66\n",
      "2019-05-17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "clf0= MLPClassifier(random_state=0 , shuffle=False , hidden_layer_sizes=(21 ))\n",
    "\n",
    "# f1_weighted_cross_validate_report(clf0, X_train, Y_train , 5)\n",
    "\n",
    "\n",
    "time_spend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:10.77\n",
      "2019-05-18\n"
     ]
    }
   ],
   "source": [
    "clf0= NearestCentroid( )\n",
    "eclf1 = clf0.fit(X_train, Y_train)\n",
    "# f1_weighted_cross_validate_report(clf0, X_train, Y_train , 5)\n",
    "\n",
    "time_spend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 18:36:27.526688\n",
      "f1_weighted_cross_validate_report: 0.7615265720976189  with cv  5\n",
      "01:26:18.92\n",
      "2019-05-17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "clf0= GradientBoostingClassifier(random_state=0 )\n",
    "\n",
    "f1_weighted_cross_validate_report(clf0, X_train, Y_train , 5)\n",
    "\n",
    "time_spend()\n",
    "\n",
    "\n",
    "play()\n",
    "t = Timer(60.0, Stop)\n",
    "t.start() # after 30 seconds, \"hello, world\" will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 20:02:46.456581\n",
      "f1_weighted_cross_validate_report: 0.7591931468687587  with cv  5\n",
      "00:34:17.95\n",
      "2019-05-17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "clf0= XGBClassifier(random_state=0 , shuffle=False  )\n",
    "\n",
    "f1_weighted_cross_validate_report(clf0, X_train, Y_train , 5)\n",
    "\n",
    "time_spend()\n",
    "\n",
    "\n",
    "play()\n",
    "t = Timer(60.0, Stop)\n",
    "t.start() # after 30 seconds, \"hello, world\" will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 20:37:04.412745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Twilio.Api.V2010.MessageInstance account_sid=AC11d404d744d8b73d522fce378769856c sid=SM64c6d25950344e60a37171e7b651753b>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import time,datetime\n",
    "# start = time.time()\n",
    "# timenow=datetime.datetime.now()\n",
    "# print(datetime.datetime.now())\n",
    "# # !pip install twilio\n",
    "# # we import the Twilio client from the dependency we just installed\n",
    "# from twilio.rest import Client\n",
    "\n",
    "# # the following line needs your Twilio Account SID and Auth Token\n",
    "# client = Client(\"AC11d404d744d8b73d522fce378769856c\", \"0ef58138ccad9b80297abf60002af3d3\")\n",
    "\n",
    "# # change the \"from_\" number to your Twilio number and the \"to\" number\n",
    "# # to the phone number you signed up for Twilio with, or upgrade your\n",
    "# # account to send SMS to any phone number\n",
    "# client.messages.create(to=\"+19714002132\", \n",
    "#                        from_=\"+19713079364\", \n",
    "#                        body=(\"Your Python process is done, Ali\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 23:01:35.179318\n",
      "00:48:56.12\n",
      "2019-05-17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "clf0=RandomForestClassifier(random_state=0, warm_start=True)\n",
    "clf1=MLPClassifier(random_state=0 , shuffle=False , hidden_layer_sizes=(21 ), warm_start=True)\n",
    "clf2=GradientBoostingClassifier(random_state=0, warm_start=True )\n",
    "clf3=XGBClassifier(random_state=0 , shuffle=False )\n",
    "clf4=ExtraTreesClassifier(random_state=0)\n",
    "clf5=BaggingClassifier(random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "                                    ('clf0', clf0) , \n",
    "                                    ('clf1', clf1) , \n",
    "                                    ('clf2', clf2), \n",
    "                                    ('clf3', clf3), \n",
    "                                    ('clf4', clf4), \n",
    "                                    ('clf5', clf5), \n",
    "                                   ])\n",
    "\n",
    "eclf1 = ensemble.fit(X_train, Y_train)\n",
    "time_spend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier\n",
    "\n",
    "XGBClassifier\n",
    "\n",
    "MLPClassifier\n",
    "\n",
    "BaggingClassifier\n",
    "\n",
    "RandomForestClassifier\n",
    "\n",
    "ExtraTreesClassifier\n",
    "\n",
    "GradientBoostingClassifier , XGBClassifier , MLPClassifier, RandomForestClassifier , ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test_prediction = pd.read_csv(path+\"merged_all_test_round.csv\")\n",
    "\n",
    "data_test_prediction.head()\n",
    "data_test = data_test_prediction.loc[:, ['date', 'time', 'o_Long', 'o_Lat', 'd_long', 'd_lat', 'distAnce', 'distance_avg', 'distance_max', 'distance_min', 'distance_std', 'price_avg', 'price_max', 'price_std', 'eta_avg', 'eta_max', 'eta_min', 'eta_std', 'mode1', 'mode2', 'mode3', 'mode4', 'mode5', 'mode6', 'mode7']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17 23:50:44.617694\n"
     ]
    }
   ],
   "source": [
    "predict = eclf1.predict(data_test)\n",
    "predict.shape, type(predict)\n",
    "# np.savetxt(path+'predict_Unknown.csv', predict, delimiter=',') \n",
    "data_test_prediction_sid=data_test_prediction.loc[:, [\"sid\"]]\n",
    "data_test_prediction_sid.head()\n",
    "data_test_prediction_sid.to_csv(path+\"testcsv.csv\", index=False)\n",
    "\n",
    "predict=pd.DataFrame({'recommend_mode':predict.tolist() })\n",
    "predict.head()\n",
    "\n",
    "final_result = pd.concat([data_test_prediction_sid,predict] , sort=False,  axis = 1)\n",
    "final_result.head()\n",
    "\n",
    "import datetime\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "print(x)\n",
    "\n",
    "final_result.to_csv(path+\"final_result_ensambel_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted_cross_validate_report(eclf1, X_train, Y_train , 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
